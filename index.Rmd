---
title: "Machine Learning Project"
author: "Wayne H"
date: "May 10, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now easy to collect a large amount of data about personal exercise activity.  Data for this project was collected from 6 participants using accelerometers on the belt, forearm, arm, and dumbell. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 

# Project Goal

The goal of this project is to come up with a model that accurately predicts the manner in which participants did the exercise.

# Download and Read Data
```{r message=FALSE}
options(warn=-1)
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
options(warn=0)
```
```{r}
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile)
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile)
}
setwd("./data")

training <- read.csv(("pml-training.csv"), na.strings=c("NA",""))

testing <- read.csv(("pml-testing.csv"), na.strings=c("NA",""))

dim(training)
```

The original data set has `r dim(training)[1]` rows and `r dim(training)[2]` variables

# Clean The Data

Cleaning the data set includes removing columns with more than 50% missing values, removing columns with little variablility, and setting the level of factor variables in the testing data set equal to the levels in the training data set.
```{r}
# Remove first column which is just an index

training <- training[c(-1)]

# Remove columns with more than 50% missing values

training<-training[, -which(colMeans(is.na(training)) > 0.5)]
dim(training)
```

After removing the index column and columns with more than 50% missing values the data set has  `r dim(training)[2]` variables.

```{r}
# Remove variables with little variability

nzv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nzv$nzv==FALSE]
dim(training)
```

After removing columns with little variablility the data set used for analysis has `r dim(training)[1]` rows and `r dim(training)[2]` variables

```{r}
# Need to make sure levels in factor variables are the same 
# in training and testing data sets
common <- intersect(names(training), names(testing)) 
for (p in common) { 
  if (class(training[[p]]) == "factor") { 
    levels(testing[[p]]) <- levels(training[[p]]) 
  } 
}

```

# Split the Training Data Set

The cleaned data set will next be split into a training data set (mytrain) containing 60% of the data and a validation data set (mytest). The mytest data set will be used for cross validation of the prediction models.

```{r}
set.seed(13579)  #set seed for reprodicibility
inTrain <- createDataPartition(training$classe, p=0.60, list=FALSE)
mytrain <- training[inTrain, ]
mytest <- training[-inTrain, ]
dim(mytrain)
dim(mytest)
```

# Prediction Models

### Tree Prediction

I will first try a simple tree prediction model using rpart on the training data, display the prediction tree with fancyRpartPlot, crossvalidate the model on the validation data, and then estimate the accuracy using confusionMatrix.

```{r}
modFit1 <- rpart(classe ~ ., data=mytrain, method="class")
fancyRpartPlot(modFit1)
predictions1 <- predict(modFit1, mytest, type = "class")
contree <- confusionMatrix(predictions1, mytest$classe)
contree
```

The estimated accuracy of the prediction for this model is `r paste0(round(contree$overall["Accuracy"] * 100, 3), "%")`. The estimated out-of-sample error is `r paste0(round(100 - contree$overall["Accuracy"] * 100, 3), "%")`.  

The accuracy of this model is too low, so next I will try a random forest model.

### Random Forest Prediction

A random forest algorithm will automatically select the most important variables.

This model will use randomForest on the training data,  crossvalidate the model on the validation data, and then estimate the accuracy using confusionMatrix.

```{r}
modFit2 <- randomForest(classe ~ ., data=mytrain)
prediction2 <- predict(modFit2, mytest, type = "class")
conrf <- confusionMatrix(prediction2, mytest$classe)
conrf
```

The estimated accuracy of the prediction for this model is `r paste0(round(conrf$overall["Accuracy"], 3), "%")`. The estimated out-of-sample error is `r paste0(round(100 - conrf$overall["Accuracy"]* 100, 3), "%")`.

As the random forest prediction accuracy is very high, no further models need to be examined.

# Prediction for the Test Data Set

Finally the random forest model will be applied to the 20 test data observations.

```{r}
# Apply the model to the test data set

predict(modFit2, testing, type = "class")
```